{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/danielshi/tw_pyspark/Code/SocialSensor\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'day': u'31',\n",
      " u'dow': u'Thu',\n",
      " u'from_user': u'SecretaryBird5',\n",
      " u'hashtag': [u'4\\u6587\\u5b57\\u3067\\u81ea\\u5206\\u3092\\u5e78\\u305b\\u306b\\u3057\\u308d'],\n",
      " u'lang': u'ja',\n",
      " u'location': [u'loc_\\u5927\\u962a\\u6c11\\u56fd'],\n",
      " u'mention': [],\n",
      " u'month': u'Jan',\n",
      " u'term': [u'\\u5fc3\\u795e\\u5b8c\\u6210',\n",
      "           u'#4\\u6587\\u5b57\\u3067\\u81ea\\u5206\\u3092\\u5e78\\u305b\\u306b\\u3057\\u308d'],\n",
      " u'time': u'14:58:06',\n",
      " u'year': u'2013'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('../output/out.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    \n",
    "pprint(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "\n",
    "filename = \"test2\"\n",
    "with open(filename, 'r') as f:\n",
    "    objects = ijson.items(f, 'created_at')\n",
    "    columns = list(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Thu Jan 31 12:58:06 +0000 2013']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import time\n",
    "#import simplejson as json\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "def parseJson(jsonfile):\n",
    "    tweets = []\n",
    "    #counter = 0\n",
    "\n",
    "    with gzip.open(jsonfile,'r') as fin:\n",
    "        for line in fin:\n",
    "            #if \"entities\" not in line:\n",
    "            #    continue\n",
    "            #print(len(line))\n",
    "            if len(line) > 1000: # check if the line is proper tweet (by observation, all valid tweets have length at least 1000.)\n",
    "                d = json.loads(line.decode(\"UTF-8\"))  # more effecient way would be check the line validity before loading\n",
    "                #print(len(d))\n",
    "                ###### 21 in windows......this line is really ad hoc...but it works...\n",
    "                #if len(d) == 21 and len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "                #print(d)\n",
    "                if len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "        \n",
    "                    ## dow = day of week.  unpacking timestamp into individual vars \n",
    "                    ##(or maybe I should do this in the next phase?  note this is still in unicode)\n",
    "                    dow, month, day, time,_,year = d['created_at'].split(\" \") \n",
    "                    ## Assign the extracted values into a new json to be stored.\n",
    "                                 ## UserName --> from_user\n",
    "                    processed = {\"from_user\":d['user']['screen_name'],\n",
    "                                 ## Split hashtag, we only want the text in hashtag, discard indices.\n",
    "                                 \"hashtag\":[hash_string['text'] for hash_string in d['entities']['hashtags']], \n",
    "                                 ## Split terms in tweet text, remove \\n and \\r\n",
    "                                 \"term\": [w.replace('\\n', '').replace('\\r','') for w in d['text'].split(\" \")], \n",
    "                                 ## append loc_ to each word in location\n",
    "                                 \"location\":['loc_' + s for s in d['user']['location'].split(\" \")],\n",
    "                                 ## mention ids\n",
    "                                 \"mention\":d['entities']['user_mentions'],\n",
    "                                 ## language\n",
    "                                 \"lang\":d['user']['lang'],\n",
    "                                 ## day of week\n",
    "                                 \"dow\":dow,\n",
    "                                 ## month in 3 letters\n",
    "                                 \"month\": month,\n",
    "                                 ## numeric day 1-31\n",
    "                                 \"day\":day,\n",
    "                                 ## hr:min:sec\n",
    "                                 \"time\":time,\n",
    "                                 ## numeric year\n",
    "                                 \"year\":year\n",
    "                                }\n",
    "                    \n",
    "                    tweets.append(processed)\n",
    "                    #tweets.append(d)\n",
    "                #if counter % 100000 == 0:\n",
    "                #    print \"processed \", counter\n",
    "                #counter+=1\n",
    "    return tweets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print(sys.argv)\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## Linux\n",
    "    #jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "    folder = sys.argv[1]    #'/mnt/f/Demoonism/Data Scientist/Spark/statuses.log.2014-07-01-00.gz'\n",
    "    # /mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_2_8/2013-02/2013-02-01\n",
    "    print(\"running folder \",folder, \"...\" )\n",
    "    ## Windows\n",
    "    #jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "    path = os.path.join(folder,'*.gz')\n",
    "    output = []\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    counter = 0\n",
    "    for jsonfilename in files[:1]:\n",
    "        #print \"processing \", counter\n",
    "        result = parseJson(jsonfilename)\n",
    "        #print result\n",
    "        #output.append(result)\n",
    "        output = output + result  ##simple list concatenation\n",
    "        counter += 1\n",
    "        if counter % 2 == 0:\n",
    "            print \"processed \", counter\n",
    "        out_filename = sys.argv[2]\n",
    "        #out_filename = 'out'\n",
    "        sec = time.time() - start_time\n",
    "        m, s = divmod(sec, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print('Pre-processing '+ folder +' takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))\n",
    "        with gzip.open(out_filename+'.json.gz', 'wb') as f:\n",
    "            json.dump(output, f,indent=0)\n",
    "#            for eachjson in output:\n",
    "#                for eachline in eachjson:\n",
    "#                    f.write(eachline)\n",
    "#                    f.write('\\n')\n",
    "                #json.dump(out, f)\n",
    "                #json.dump(output, f,indent=1)\n",
    "                #f.write(out)\n",
    "                #f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
