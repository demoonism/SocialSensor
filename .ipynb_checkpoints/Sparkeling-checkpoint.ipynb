{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.feature import VectorAssemblerg\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw = sc.textFile('/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Tweet_Output/2013-02')\n",
    "\n",
    "#data_raw = sc.textFile('/resources/data/2013-06') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data_raw.map(lambda line: json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature = data.map(lambda line:  line['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stamp = u'2013 Jun 09 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_error(line):\n",
    "    return \"ERROR\" in line\n",
    "errors = data.filter(is_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parserHashTag(r):\n",
    "    r['hashtag']\n",
    "    return r['hashtag']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtagRDD = data.flatMap(parserHashTag).map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "count = hashtagRDD.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'gameinsight', 179596),\n",
       " (u'EMABiggestFans1D', 122834),\n",
       " (u'EMABiggestFansJustinBieber', 121050),\n",
       " (u'androidgames', 94927),\n",
       " (u'android', 88565),\n",
       " (u'KCAArgentina', 88061),\n",
       " (u'RT', 86947),\n",
       " (u'ipadgames', 60352),\n",
       " (u'TeamFollowBack', 57245),\n",
       " (u'ipad', 53629)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted\n",
    "count.takeOrdered(10, key=lambda x: -x[1])   # x[1] means sort by value.\n",
    "\n",
    "## asending order takeOrdered(10, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'iloveyou', 1370768309.0), (u'missyou', 1370768309.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parserMin(r):\n",
    "    pair = []\n",
    "    for hashtag in r['hashtag']:\n",
    "        pair.append((hashtag, r['HashTag_Birthday']))\n",
    "        \n",
    "    return pair\n",
    "\n",
    "def getMin(a,b):\n",
    "    return a if a < b else b\n",
    "parserMin(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hashtag_birthday = data.flatMap(parserMin).reduceByKey(getMin) # reduce tells you how to compare values in two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'\\u0639\\u0644\\u064a_\\u0628\\u0627\\u0644\\u0639\\u0627\\u0641\\u064a\\u0647',\n",
       "  1378764525.0),\n",
       " (u'Annnttttt', 1413283609.0),\n",
       " (u'\\u0627\\u0644\\u0635\\u0651\\u0648\\u0631\\u0629_\\u062a\\u064e\\u062d\\u0643\\u064e\\u064a',\n",
       "  1378762260.0),\n",
       " (u'lv167200837', 1390755382.0),\n",
       " (u'lutoko', 1413038549.0),\n",
       " (u'\\u5c71\\u53e3\\u7f8e\\u6c99', 1390899759.0),\n",
       " (u'LustBot', 1370505198.0),\n",
       " (u'Xarelto', 1378815369.0),\n",
       " (u'laverdadfiscal', 1414359562.0),\n",
       " (u'DNBJPN', 1389810504.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hashtag_birthday.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample2 = data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parserAll(r):\n",
    "    try:\n",
    "        x=Row(Birthday=float(r['HashTag_Birthday']),\\\n",
    "          UserName= str(r['from_user']), \\\n",
    "          Hashtag= r['hashtag'],\\\n",
    "          Language=r['lang'],\\\n",
    "          Location=r['location'], \\\n",
    "          Mention=r['mention'],\\\n",
    "          Term=r['term'])  \n",
    "    except:\n",
    "        x=None  \n",
    "    return x\n",
    "\n",
    "rowRDD = data.map(lambda r: parserAll(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Birthday=1370768309.0, Hashtag=[u'iloveyou', u'missyou'], Language=u'en', Location=[u'loc_Indonesia'], Mention=[u'aku_jujur'], Term=[u'RT', u'@aku_jujur:', u'Jika', u'bisa,', u'berikan', u'aku', u'mimpi', u'tentangnya,', u'dan', u'jika', u'berhasil', u'tolong', u'jangan', u'bangunkan', u'aku', u'#iloveyou', u'#missyou'], UserName='Anasya_Bella')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data is already a DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-71e5f2862b04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFeature_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/notebook/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data is already a DataFrame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data is already a DataFrame"
     ]
    }
   ],
   "source": [
    "Feature_df = sqlContext.createDataFrame(rowRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flatten_json\n",
      "  Downloading flatten_json-0.1.5.tar.gz\n",
      "Building wheels for collected packages: flatten-json\n",
      "  Running setup.py bdist_wheel for flatten-json ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/notebook/.cache/pip/wheels/e4/ab/04/38966a4fb96e02c81dcb926a43a787f437c6111f1c69b0353a\n",
      "Successfully built flatten-json\n",
      "Installing collected packages: flatten-json\n",
      "Successfully installed flatten-json-0.1.5\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flatten_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'HashTag_Birthday': 1370768309.0,\n",
       " u'from_user': u'Anasya_Bella',\n",
       " 'hashtag_0': u'iloveyou',\n",
       " 'hashtag_1': u'missyou',\n",
       " u'lang': u'en',\n",
       " 'location_0': u'loc_Indonesia',\n",
       " 'mention_0': u'aku_jujur',\n",
       " 'term_0': u'RT',\n",
       " 'term_1': u'@aku_jujur:',\n",
       " 'term_10': u'berhasil',\n",
       " 'term_11': u'tolong',\n",
       " 'term_12': u'jangan',\n",
       " 'term_13': u'bangunkan',\n",
       " 'term_14': u'aku',\n",
       " 'term_15': u'#iloveyou',\n",
       " 'term_16': u'#missyou',\n",
       " 'term_2': u'Jika',\n",
       " 'term_3': u'bisa,',\n",
       " 'term_4': u'berikan',\n",
       " 'term_5': u'aku',\n",
       " 'term_6': u'mimpi',\n",
       " 'term_7': u'tentangnya,',\n",
       " 'term_8': u'dan',\n",
       " 'term_9': u'jika'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flatten_json as fj\n",
    "fj.flatten_json(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(HashTag_Birthday=1370768309.0, from_user=u'Anasya_Bella', hashtag=[u'iloveyou', u'missyou'], lang=u'en', location=[u'loc_Indonesia'], mention=[u'aku_jujur'], term=[u'RT', u'@aku_jujur:', u'Jika', u'bisa,', u'berikan', u'aku', u'mimpi', u'tentangnya,', u'dan', u'jika', u'berhasil', u'tolong', u'jangan', u'bangunkan', u'aku', u'#iloveyou', u'#missyou'])]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Feature_df = df2\n",
    "Feature_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat1 = Feature_df.withColumn('Hashtag', explode('Hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat1.registerTempTable(\"FeatureDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(HashTag_Birthday=1370768309.0, from_user=u'Anasya_Bella', Hashtag=u'iloveyou', lang=u'en', location=[u'loc_Indonesia'], mention=[u'aku_jujur'], term=[u'RT', u'@aku_jujur:', u'Jika', u'bisa,', u'berikan', u'aku', u'mimpi', u'tentangnya,', u'dan', u'jika', u'berhasil', u'tolong', u'jangan', u'bangunkan', u'aku', u'#iloveyou', u'#missyou'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(HashTag_Count=2217643)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approxCountDistinct\n",
    "flat1.agg(countDistinct(flat1.Hashtag).alias('HashTag_Count')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(From_Count=5917584)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Feature_df.agg(countDistinct(Feature_df.from_user).alias('From_Count')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat2 = Feature_df.withColumn('mention', explode('mention'))  ## bug...? mention return different countdistinct from_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(From_Count=2503011)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat2.agg(countDistinct(flat2.mention).alias('From_Count')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eDF = sqlContext.createDataFrame([Row(a=1, intlist=[1,2,3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat = eDF.withColumn('intlist', explode('intlist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|  a|intlist|\n",
      "+---+-------+\n",
      "|  1|      1|\n",
      "|  1|      2|\n",
      "|  1|      3|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature_df.registerTempTable(\"FeatureDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedDelay = sqlContext.sql(\"SELECT Origin, count(*) conFlight,avg(DepDelay) delay \\\n",
    "                                FROM FeatureDF \\\n",
    "                                GROUP BY Origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize( [(0, 2.), (0, 4.), (1, 0.), (1, 10.), (1, 20.)] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 2.0), 1), ((0, 4.0), 1), ((1, 0.0), 1), ((1, 10.0), 1), ((1, 20.0), 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda value: (value, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sumCount = data.combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "averageByKey = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count))\n",
    "\n",
    "print averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "       x=Row(Birthday=float(r['HashTag_Birthday']),\\\n",
    "          UserName= str(r['from_user']), \\\n",
    "          Hashtag= r['hashtag'],\\\n",
    "          Language=r['lang'],\\\n",
    "          Location=r['location'], \\\n",
    "          Mention=r['mention'],\\\n",
    "          Term=r['term']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"hashtag\", StringType(), True),\n",
    "    StructField(\"HashTag_Birthday\", FloatType(), True),\n",
    "    StructField(\"from_user\", StringType(), True),\n",
    "    StructField(\"lang\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"mention\", StringType(), True),\n",
    "    StructField(\"term\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HashTag_Birthday: double (nullable = true)\n",
      " |-- from_user: string (nullable = true)\n",
      " |-- hashtag: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- location: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- mention: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- term: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = sqlContext.read.json('/resources/data/ProcessesTweeter')\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(HashTag_Birthday=1370768309.0, from_user=u'Anasya_Bella', hashtag=[u'iloveyou', u'missyou'], lang=u'en', location=[u'loc_Indonesia'], mention=[u'aku_jujur'], term=[u'RT', u'@aku_jujur:', u'Jika', u'bisa,', u'berikan', u'aku', u'mimpi', u'tentangnya,', u'dan', u'jika', u'berhasil', u'tolong', u'jangan', u'bangunkan', u'aku', u'#iloveyou', u'#missyou'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record = {\n",
    "    'first_name': 'nadbor',\n",
    "    'last_name': 'drozd',\n",
    "    'occupation': 'data scientist',\n",
    "    'children': [\n",
    "        {\n",
    "            'name': 'Lucja',\n",
    "            'age': 3,\n",
    "            'likes cold showers': True\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "schema = StructType([\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"occupation\", StringType(), True),\n",
    "        StructField(\"children\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"age\", IntegerType(), True),\n",
    "                StructField(\"likes cold schowers\", BooleanType(), True)\n",
    "            ])\n",
    "        ), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
