{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-dc3de390f0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ASCII\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# more effecient way would be check the line validity before loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m24\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hashtags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m## explicit booleaness to check is a list is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mA:\\Anaconda2\\lib\\json\\__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mA:\\Anaconda2\\lib\\json\\decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[1;32m    364\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extra data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "## Linux\n",
    "#jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "\n",
    "## Windows\n",
    "jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "\n",
    "tweets = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with gzip.open(jsonfilename,'r') as fin:\n",
    "    counter = 0\n",
    "    for line in fin:\n",
    "        if counter == 3:\n",
    "            print(line)\n",
    "        d = json.loads(line.decode(\"ASCII\"))  # more effecient way would be check the line validity before loading\n",
    "        if len(d) == 24 and len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "            tweets.append(d)\n",
    "            if counter % 50000 ==0:\n",
    "                print counter\n",
    "        counter+=1\n",
    "        \n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'contributors': None,\n",
       " u'coordinates': None,\n",
       " u'created_at': u'Mon Jun 30 14:01:25 +0000 2014',\n",
       " u'entities': {u'hashtags': [{u'indices': [57, 62], u'text': u'bb16'}],\n",
       "  u'symbols': [],\n",
       "  u'urls': [],\n",
       "  u'user_mentions': [{u'id': 570393739,\n",
       "    u'id_str': u'570393739',\n",
       "    u'indices': [0, 15],\n",
       "    u'name': u'victoria',\n",
       "    u'screen_name': u'Victoriaaaa729'}]},\n",
       " u'favorite_count': 0,\n",
       " u'favorited': False,\n",
       " u'filter_level': u'medium',\n",
       " u'geo': None,\n",
       " u'id': 483611266549039104L,\n",
       " u'id_str': u'483611266549039104',\n",
       " u'in_reply_to_screen_name': u'Victoriaaaa729',\n",
       " u'in_reply_to_status_id': 483600116461633536L,\n",
       " u'in_reply_to_status_id_str': u'483600116461633536',\n",
       " u'in_reply_to_user_id': 570393739,\n",
       " u'in_reply_to_user_id_str': u'570393739',\n",
       " u'lang': u'en',\n",
       " u'place': None,\n",
       " u'retweet_count': 0,\n",
       " u'retweeted': False,\n",
       " u'source': u'<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       " u'text': u\"@Victoriaaaa729 thanks Victoria! Miss and love you also! #bb16 can't handle us\\U0001f61c\",\n",
       " u'timestamp': 1404136885667L,\n",
       " u'truncated': False,\n",
       " u'user': {u'contributors_enabled': False,\n",
       "  u'created_at': u'Fri Aug 30 15:02:14 +0000 2013',\n",
       "  u'default_profile': True,\n",
       "  u'default_profile_image': False,\n",
       "  u'description': u'Cardinal lax #3',\n",
       "  u'favourites_count': 1284,\n",
       "  u'follow_request_sent': None,\n",
       "  u'followers_count': 273,\n",
       "  u'following': None,\n",
       "  u'friends_count': 195,\n",
       "  u'geo_enabled': False,\n",
       "  u'id': 1712977022,\n",
       "  u'id_str': u'1712977022',\n",
       "  u'is_translation_enabled': False,\n",
       "  u'is_translator': False,\n",
       "  u'lang': u'en',\n",
       "  u'listed_count': 0,\n",
       "  u'location': u'',\n",
       "  u'name': u'han',\n",
       "  u'notifications': None,\n",
       "  u'profile_background_color': u'C0DEED',\n",
       "  u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  u'profile_background_tile': False,\n",
       "  u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/1712977022/1399501210',\n",
       "  u'profile_image_url': u'http://pbs.twimg.com/profile_images/479661726925737985/Pc_r25Zt_normal.jpeg',\n",
       "  u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/479661726925737985/Pc_r25Zt_normal.jpeg',\n",
       "  u'profile_link_color': u'0084B4',\n",
       "  u'profile_sidebar_border_color': u'C0DEED',\n",
       "  u'profile_sidebar_fill_color': u'DDEEF6',\n",
       "  u'profile_text_color': u'333333',\n",
       "  u'profile_use_background_image': True,\n",
       "  u'protected': False,\n",
       "  u'screen_name': u'hannahcapestany',\n",
       "  u'statuses_count': 2020,\n",
       "  u'time_zone': u'Eastern Time (US & Canada)',\n",
       "  u'url': None,\n",
       "  u'utc_offset': -14400,\n",
       "  u'verified': False}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with gzip.GzipFile(jsonfilename, 'r') as fin:        # 4. gzip\n",
    "#    json_bytes = fin.read()                          # 3. bytes (i.e. ascii)\n",
    "#    json_str = json_bytes.decode('ascii')            # 2. string\n",
    "#    data = json.loads(json_str)                      # 1. data\n",
    "\n",
    "#data\n",
    "data = tweets[211]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags =  data['entities']['hashtags']\n",
    "hastag_string = [hash_string['text'] for hash_string in hashtags]   # we only want the text in hashtag.\n",
    "mentions =  data['entities']['user_mentions']\n",
    "term = data['text']\n",
    "from_user = data['user']['screen_name']\n",
    "location = data['user']['location']\n",
    "timestamp = data['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dow, month, day, time,_,year = timestamp.split(\" \") ## dow = day of week.  unpacking timestamp into individual vars (or maybe I should do this at the end?  note this is still in unicode)\n",
    "words = term .split(\" \")\n",
    "location_prefix =  ['loc_' + s for s in location.split(\" \")] # append loc_ to each word in location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2106ac42c2a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m              \u001b[1;34m\"year\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             }\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "processed = {\"from_user\":from_user, \n",
    "             \"hashtag\":hastag_string, \n",
    "             \"term\": words, \n",
    "             \"location\":location_prefix, \n",
    "             \"mention\":mentions,\n",
    "             \"dow\":dow,\n",
    "             \"month\": month,\n",
    "             \"day\":day,\n",
    "             \"time\":time,\n",
    "             \"year\":year\n",
    "            }\n",
    "output.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Playa Sol Arena \\nEn Cartagena de Indias #Amigos'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es', -52.2901713848114)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.classify(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def is_greek(term):\n",
    "    return 'GREEK' in unicodedata.name(term.strip()[0])\n",
    "\n",
    "def is_english(term):\n",
    "    return 'ENGLISH' in unicodedata.name(term.strip()[0])\n",
    "\n",
    "def is_hebrew(term):\n",
    "    return 'HEBREW' in unicodedata.name(term.strip()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_english(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(processed, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'es'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "detect(words[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseJson(jsonfile):\n",
    "    tweets = []\n",
    "    with gzip.open(jsonfile,'r') as fin:\n",
    "        counter = 0\n",
    "        for line in fin:\n",
    "            d = json.loads(line)  # more effecient way would be check the line validity before loading\n",
    "            if len(d) == 24 and len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "                \n",
    "                ## dow = day of week.  unpacking timestamp into individual vars \n",
    "                ##(or maybe I should do this in the next phase?  note this is still in unicode)\n",
    "                dow, month, day, time,_,year = d['created_at'].split(\" \") \n",
    "                ## Assign the extracted values into a new json to be stored.\n",
    "                             ## UserName --> from_user\n",
    "                processed = {\"from_user\":d['user']['screen_name'],\n",
    "                             ## Split hashtag, we only want the text in hashtag, discard indices.\n",
    "                             \"hashtag\":[hash_string['text'] for hash_string in d['entities']['hashtags']], \n",
    "                             ## Split terms in tweet text\n",
    "                             \"term\": d['text'].split(\" \"), \n",
    "                             ## append loc_ to each word in location\n",
    "                             \"location\":['loc_' + s for s in d['user']['location'].split(\" \")],\n",
    "                             ## mention ids\n",
    "                             \"mention\":d['entities']['user_mentions'],\n",
    "                             ## language\n",
    "                             \"lang\":d['lang'],\n",
    "                             ## day of week\n",
    "                             \"dow\":dow,\n",
    "                             ## month in 3 letters\n",
    "                             \"month\": month,\n",
    "                             ## numeric day 1-31\n",
    "                             \"day\":day,\n",
    "                             ## hr:min:sec\n",
    "                             \"time\":time,\n",
    "                             ## numeric year\n",
    "                             \"year\":year\n",
    "                            }\n",
    "                tweets.append(processed)\n",
    "                #tweets.append(d)\n",
    "                if counter % 10000 == 0:\n",
    "                    print \"processed \", counter\n",
    "            counter+=1\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed  80000\n",
      "processed  100000\n",
      "processed  160000\n",
      "30.4680001736\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "## Linux\n",
    "#jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "\n",
    "## Windows\n",
    "jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "t0 = time.time()\n",
    "result = parseJson(jsonfilename)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "\n",
    "print(total)\n",
    "\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.7.zip (998kB)\n",
      "Requirement already satisfied: six in a:\\anaconda2\\lib\\site-packages (from langdetect)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Running setup.py bdist_wheel for langdetect: started\n",
      "  Running setup.py bdist_wheel for langdetect: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\xians\\AppData\\Local\\pip\\Cache\\wheels\\6f\\8c\\3b\\ffa8151e27effd7de2a7d3194650d78fe6e4d4a3c175a74867\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.7\n"
     ]
    }
   ],
   "source": [
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
