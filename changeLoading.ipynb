{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import time\n",
    "#import simplejson as json\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanText(x):\n",
    "    return not (x.startswith(\"#\")|x.startswith(\"@\")|x.startswith(\"http:\")| (x == '\\n')| (x == '\\r'))\n",
    "\n",
    "def getUnixTimeStamp(stamp):\n",
    "    d = datetime.strptime(stamp,'%a %b %d %H:%M:%S +0000 %Y')\n",
    "    unixtime = time.mktime(d.timetuple())\n",
    "    return unixtime\n",
    "\n",
    "\n",
    "def parseJson(jsonfile):\n",
    "    tweets = []\n",
    "\n",
    "    with gzip.open(jsonfile,'r') as fin:\n",
    "        for line in fin:\n",
    "\n",
    "            if len(line) > 1000: # check if the line is proper tweet (by observation, all valid tweets have length at least 1000.)\n",
    "                d = json.loads(line.decode(\"UTF-8\"))  # more effecient way would be check the line validity before loading\n",
    "\n",
    "                if len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "        \n",
    "                    ## dow = day of week.  unpacking timestamp into individual vars \n",
    "                    ##(or maybe I should do this in the next phase?  note this is still in unicode)\n",
    "                    dow, month, day, time,_,year = d['created_at'].split(\" \") \n",
    "                    ## Assign the extracted values into a new json to be stored.\n",
    "                                 ## UserName --> from_user\n",
    "                    hash_list = ['#'+hash_string['text'] for hash_string in d['entities']['hashtags']]\n",
    "                    mention_list = d['entities']['user_mentions']\n",
    "                    #term_list = list(set(d['text'].split(\" \")) - set(hash_list))\n",
    "                    term_list = filter(cleanText, d['text'].split(\" \"))\n",
    "                    \n",
    "                    processed = {\"from_user\":d['user']['screen_name'],\n",
    "                                 ## Split hashtag, we only want the text in hashtag, discard indices.\n",
    "                                 \"hashtag\":hash_list, \n",
    "                                 ## Split terms in tweet text, remove \\n and \\r\n",
    "                                 #\"term\": [w.replace('\\n', '').replace('\\r','') for w in d['text'].split(\" \")], \n",
    "                                 \"term\": term_list, \n",
    "                                 ## append loc_ to each word in location\n",
    "                                 #\"location\":['loc_' + s for s in d['user']['location'].split(\" \")],\n",
    "                                 \"location\":d['user']['location']\n",
    "                                 ## mention ids\n",
    "                                 \"mention\":d['entities']['user_mentions'],\n",
    "                                 ## language\n",
    "                                 \"lang\":d['user']['lang'],\n",
    "                                 ## day of week\n",
    "                                 \"dow\":dow,\n",
    "                                 ## month in 3 letters\n",
    "                                 \"month\": month,\n",
    "                                 ## numeric day 1-31\n",
    "                                 \"day\":day,\n",
    "                                 ## hr:min:sec\n",
    "                                 \"time\":time,\n",
    "                                 ## numeric year\n",
    "                                 \"year\":year,\n",
    "                                 \"created_at\":getUnixTimeStamp(d['created_at'])\n",
    "                                }\n",
    "                    tweets.append(processed)\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('running folder ', 'F:/Demoonism/DataScientist/Spark/data/batch1', '...')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'bool' and 'unicode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9472758759f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mjsonfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparseJson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsonfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresult\u001b[0m  \u001b[1;31m##simple list concatenation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-fbe685c05731>\u001b[0m in \u001b[0;36mparseJson\u001b[0;34m(jsonfile)\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mmention_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_mentions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[1;31m#term_list = list(set(d['text'].split(\" \")) - set(hash_list))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mterm_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     processed = {\"from_user\":d['user']['screen_name'],\n",
      "\u001b[0;32m<ipython-input-60-fbe685c05731>\u001b[0m in \u001b[0;36mcleanText\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m|\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"@\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m|\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"http:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m|\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mu'\\n'\u001b[0m\u001b[1;33m|\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mu'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparseJson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsonfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'bool' and 'unicode'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print(sys.argv)\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## Linux\n",
    "    #jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "    folder = 'F:/Demoonism/DataScientist/Spark/data/batch1' #sys.argv[1]    #'/mnt/f/Demoonism/Data Scientist/Spark/statuses.log.2014-07-01-00.gz'\n",
    "    # /mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_2_8/2013-02/2013-02-01\n",
    "    print(\"running folder \",folder, \"...\" )\n",
    "    ## Windows\n",
    "    #jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "    path = os.path.join(folder,'*.gz')\n",
    "    output = []\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    counter = 0\n",
    "    for jsonfilename in files:\n",
    "\n",
    "        result = parseJson(jsonfilename)\n",
    "        output = output + result  ##simple list concatenation\n",
    "        counter += 1\n",
    "        if counter % 2 == 0:\n",
    "            print \"processed \", counter\n",
    "    out_filename = sys.argv[2]\n",
    "    sec = time.time() - start_time\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print('Pre-processing '+ folder +' takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))\n",
    "    with gzip.open(out_filename+'.json.gz', 'wb') as f:\n",
    "        for eachjson in output:\n",
    "            json.dump(eachjson, f) # works for json, cannot write(json)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('running folder ', 'F:/Demoonism/DataScientist/Spark/data/batch1', '...')\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print(sys.argv)\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## Linux\n",
    "    #jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "    folder = 'F:/Demoonism/DataScientist/Spark/data/batch1' #sys.argv[1]    #'/mnt/f/Demoonism/Data Scientist/Spark/statuses.log.2014-07-01-00.gz'\n",
    "    # /mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_2_8/2013-02/2013-02-01\n",
    "    print(\"running folder \",folder, \"...\" )\n",
    "    ## Windows\n",
    "    #jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "    path = os.path.join(folder,'*.gz')\n",
    "    output = []\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    counter = 0\n",
    "    for jsonfilename in files:\n",
    "\n",
    "        result = parseJson(jsonfilename)\n",
    "        output = output + result  ##simple list concatenation\n",
    "        counter += 1\n",
    "        if counter % 2 == 0:\n",
    "            print \"processed \", counter\n",
    "            \n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtag': [u'#SlowJama', u'#HarlemNights'], 'year': u'2014', 'month': u'Jun', 'mention': [], 'from_user': u'Gorgeous_Lady2', 'day': u'30', 'lang': u'en', 'term': [u'This', u'Thursday', u'18+', u'(Biggest', u'Lingerie/Pajama', u'Party)', u'Free', u'Til', u'12', u'+', u'Ladies', u'FREE', u'in', u'Lingerie', u'x11'], 'created_at': 1404151200.0, 'location': [u'loc_@Gorgeous_Lady2', u'loc_follows', u'loc_you'], 'time': u'14:00:00', 'dow': u'Mon'}\n"
     ]
    }
   ],
   "source": [
    "print(output[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'http://t.co/RFPXOc1OEe'.startswith('http:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2014, 6, 30, 14, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#datetime_object = datetime.strptime(output[0]['created_at'],'%a %b %d %H:%M:%S %Z %Y')\n",
    "\n",
    "d = datetime.strptime(output[4]['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unixtime = time.mktime(d.timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2014, tm_mon=6, tm_mday=30, tm_hour=14, tm_min=0, tm_sec=0, tm_wday=0, tm_yday=181, tm_isdst=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.timetuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'utctimetuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-485e0efa3048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtimestamp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalendar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimegm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutctimetuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'utctimetuple'"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "from datetime import datetime\n",
    "d = datetime.utcnow()\n",
    "timestamp=calendar.timegm(output[0]['created_at'].encode('ascii','ignore').utctimetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1404151200.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really love that shirt at\n",
      "Titanic tragedy could have been prevented Economic Times Telegraph co ukTitanic tragedy could have been preve\n",
      "I am at Starbucks 7419 3rd ave at 75th Brooklyn\n"
     ]
    }
   ],
   "source": [
    "import re,string\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "tests = [\n",
    "    \"@peter I really love that shirt at #Macy. http://bet.ly//WjdiW4\",\n",
    "    \"@shawn Titanic tragedy could have been prevented Economic Times: Telegraph.co.ukTitanic tragedy could have been preve... http://bet.ly/tuN2wx\",\n",
    "    \"I am at Starbucks http://4sh.com/samqUI (7419 3rd ave, at 75th, Brooklyn)\",\n",
    "]\n",
    "for t in tests:\n",
    "    print(strip_all_entities(strip_links(t)))\n",
    "\n",
    "\n",
    "#'I really love that shirt at'\n",
    "#'Titanic tragedy could have been prevented Economic Times Telegraph co ukTitanic tragedy could have been preve'\n",
    "#'I am at Starbucks 7419 3rd ave at 75th Brooklyn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_1 = ['a', 'big', 'list']\n",
    "list_2 = ['another', 'big', 'list']\n",
    "\n",
    "intersection = list(set(list_1) - set(list_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'equals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8e0a2920d222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;34m'x'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'equals'"
     ]
    }
   ],
   "source": [
    "'x'.equals('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
