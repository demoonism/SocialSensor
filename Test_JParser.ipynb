{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19617605209350586"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "#import simplejson as json\n",
    "## Linux\n",
    "#jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "\n",
    "## Windows\n",
    "jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "\n",
    "tweets = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with gzip.open('../output/out.json.gz','r') as fin:\n",
    "    counter = 0\n",
    "    for line in fin:\n",
    "        d = json.loads(line.decode(\"UTF-8\"))  # more effecient way would be check the line validity before loading\n",
    "        if len(d) == 24 and len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "            tweets.append(d)\n",
    "        if counter % 50000 ==0:\n",
    "            print counter\n",
    "        counter+=1\n",
    "        \n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2587130069732666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "#import simplejson as json\n",
    "## Linux\n",
    "#jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "\n",
    "## Windows\n",
    "jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "\n",
    "tweets = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with gzip.open('../output/out.json.gz','r') as fin:\n",
    "    for line in fin:\n",
    "        d = json.loads(line.decode(\"UTF-8\"))  # more effecient way would be check the line validity before loading\n",
    "        tweets.append(d)\n",
    "        \n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6586"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'day': u'31',\n",
       " u'dow': u'Thu',\n",
       " u'from_user': u'marcosarmandoh',\n",
       " u'hashtag': [u'StandUpComedy', u'Humor', u'Magia'],\n",
       " u'lang': u'es',\n",
       " u'location': [u'loc_Puerto',\n",
       "  u'loc_La',\n",
       "  u'loc_Cruz',\n",
       "  u'loc_/',\n",
       "  u'loc_Venezuela'],\n",
       " u'mention': [{u'id': 248258795,\n",
       "   u'id_str': u'248258795',\n",
       "   u'indices': [3, 14],\n",
       "   u'name': u'Judibeth Medina',\n",
       "   u'screen_name': u'Judimedina'},\n",
       "  {u'id': 198985923,\n",
       "   u'id_str': u'198985923',\n",
       "   u'indices': [19, 31],\n",
       "   u'name': u'TimbaleroConcertHall',\n",
       "   u'screen_name': u'TimbaleroCH'},\n",
       "  {u'id': 101240297,\n",
       "   u'id_str': u'101240297',\n",
       "   u'indices': [36, 50],\n",
       "   u'name': u'Carlos Castro',\n",
       "   u'screen_name': u'ElNegroCastro'},\n",
       "  {u'id': 198985923,\n",
       "   u'id_str': u'198985923',\n",
       "   u'indices': [81, 93],\n",
       "   u'name': u'TimbaleroConcertHall',\n",
       "   u'screen_name': u'TimbaleroCH'}],\n",
       " u'month': u'Jan',\n",
       " u'term': [u'RT',\n",
       "  u'@Judimedina:',\n",
       "  u'RT',\n",
       "  u'@TimbaleroCH:',\n",
       "  u'RT',\n",
       "  u'@ElNegroCastro:',\n",
       "  u'Esta',\n",
       "  u'Noche',\n",
       "  u'#StandUpComedy',\n",
       "  u'en',\n",
       "  u'@TimbaleroCH',\n",
       "  u'ENTRADA',\n",
       "  u'GRATIS!',\n",
       "  u'#Humor',\n",
       "  u'y',\n",
       "  u'#Magia',\n",
       "  u'en',\n",
       "  u'...',\n",
       "  u'htt',\n",
       "  u'...'],\n",
       " u'time': u'14:58:05',\n",
       " u'year': u'2013'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'contributors': None,\n",
       " u'coordinates': None,\n",
       " u'created_at': u'Mon Jun 30 14:03:18 +0000 2014',\n",
       " u'entities': {u'hashtags': [{u'indices': [5, 17], u'text': u'\\xd1uclyArias_'},\n",
       "   {u'indices': [81, 93], u'text': u'\\xd1uclyArias_'}],\n",
       "  u'symbols': [],\n",
       "  u'urls': [],\n",
       "  u'user_mentions': []},\n",
       " u'favorite_count': 0,\n",
       " u'favorited': False,\n",
       " u'filter_level': u'medium',\n",
       " u'geo': None,\n",
       " u'id': 483611740475650048L,\n",
       " u'id_str': u'483611740475650048',\n",
       " u'in_reply_to_screen_name': None,\n",
       " u'in_reply_to_status_id': None,\n",
       " u'in_reply_to_status_id_str': None,\n",
       " u'in_reply_to_user_id': None,\n",
       " u'in_reply_to_user_id_str': None,\n",
       " u'lang': u'es',\n",
       " u'place': None,\n",
       " u'retweet_count': 0,\n",
       " u'retweeted': False,\n",
       " u'source': u'<a href=\"http://twitterfeed.com\" rel=\"nofollow\">twitterfeed</a>',\n",
       " u'text': u'( \\u2665_ #\\xd1uclyArias_\\u2665 ) Presentan lapiceros que har\\xe1n la funci\\xf3n de tu dedito  ( \\u2665_ #\\xd1uclyArias_\\u2665 )',\n",
       " u'timestamp': 1404136998665L,\n",
       " u'truncated': False,\n",
       " u'user': {u'contributors_enabled': False,\n",
       "  u'created_at': u'Sun Jul 22 18:43:22 +0000 2012',\n",
       "  u'default_profile': False,\n",
       "  u'default_profile_image': False,\n",
       "  u'description': u'Solo No Se Qe Aser -Pero Cuando Estoy Con Tigo -Se Me olvida Too. Si Te ise Llora Perdoname Qe Llo No Qiero Qe Llores Mas. Si Nesecito Tu Perdon Qe No Puedo Mas',\n",
       "  u'favourites_count': 18,\n",
       "  u'follow_request_sent': None,\n",
       "  u'followers_count': 631,\n",
       "  u'following': None,\n",
       "  u'friends_count': 547,\n",
       "  u'geo_enabled': True,\n",
       "  u'id': 711096355,\n",
       "  u'id_str': u'711096355',\n",
       "  u'is_translation_enabled': False,\n",
       "  u'is_translator': False,\n",
       "  u'lang': u'es',\n",
       "  u'listed_count': 8,\n",
       "  u'location': u'https://www.facebook.com/ander',\n",
       "  u'name': u'---Perdoname Plis---',\n",
       "  u'notifications': None,\n",
       "  u'profile_background_color': u'E01212',\n",
       "  u'profile_background_image_url': u'http://pbs.twimg.com/profile_background_images/482299501068050434/MMj3kfYn.jpeg',\n",
       "  u'profile_background_image_url_https': u'https://pbs.twimg.com/profile_background_images/482299501068050434/MMj3kfYn.jpeg',\n",
       "  u'profile_background_tile': True,\n",
       "  u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/711096355/1403013333',\n",
       "  u'profile_image_url': u'http://pbs.twimg.com/profile_images/482974284252327936/ZiWnNqfj_normal.jpeg',\n",
       "  u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/482974284252327936/ZiWnNqfj_normal.jpeg',\n",
       "  u'profile_link_color': u'E60909',\n",
       "  u'profile_sidebar_border_color': u'FFFFFF',\n",
       "  u'profile_sidebar_fill_color': u'EFEFEF',\n",
       "  u'profile_text_color': u'333333',\n",
       "  u'profile_use_background_image': True,\n",
       "  u'protected': False,\n",
       "  u'screen_name': u'TeloamoArias',\n",
       "  u'statuses_count': 72290,\n",
       "  u'time_zone': u'International Date Line West',\n",
       "  u'url': None,\n",
       "  u'utc_offset': -39600,\n",
       "  u'verified': False}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with gzip.GzipFile(jsonfilename, 'r') as fin:        # 4. gzip\n",
    "#    json_bytes = fin.read()                          # 3. bytes (i.e. ascii)\n",
    "#    json_str = json_bytes.decode('ascii')            # 2. string\n",
    "#    data = json.loads(json_str)                      # 1. data\n",
    "\n",
    "#data\n",
    "data = tweets[500]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags =  data['entities']['hashtags']\n",
    "hastag_string = [hash_string['text'] for hash_string in hashtags]   # we only want the text in hashtag.\n",
    "mentions =  data['entities']['user_mentions']\n",
    "term = data['text']\n",
    "from_user = data['user']['screen_name']\n",
    "location = data['user']['location']\n",
    "timestamp = data['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dow, month, day, time,_,year = timestamp.split(\" \") ## dow = day of week.  unpacking timestamp into individual vars (or maybe I should do this at the end?  note this is still in unicode)\n",
    "words = term .split(\" \")\n",
    "location_prefix =  ['loc_' + s for s in location.split(\" \")] # append loc_ to each word in location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2106ac42c2a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m              \u001b[1;34m\"year\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             }\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "processed = {\"from_user\":from_user, \n",
    "             \"hashtag\":hastag_string, \n",
    "             \"term\": words, \n",
    "             \"location\":location_prefix, \n",
    "             \"mention\":mentions,\n",
    "             \"dow\":dow,\n",
    "             \"month\": month,\n",
    "             \"day\":day,\n",
    "             \"time\":time,\n",
    "             \"year\":year\n",
    "            }\n",
    "output.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Playa Sol Arena \\nEn Cartagena de Indias #Amigos'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es', -52.2901713848114)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.classify(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def is_greek(term):\n",
    "    return 'GREEK' in unicodedata.name(term.strip()[0])\n",
    "\n",
    "def is_english(term):\n",
    "    return 'ENGLISH' in unicodedata.name(term.strip()[0])\n",
    "\n",
    "def is_hebrew(term):\n",
    "    return 'HEBREW' in unicodedata.name(term.strip()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_english(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(processed, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'es'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "detect(words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseJson(jsonfile):\n",
    "    tweets = []\n",
    "    with gzip.open(jsonfile,'r') as fin:\n",
    "        counter = 0\n",
    "        for line in fin:\n",
    "            if \"entities\" not in line:\n",
    "                continue\n",
    "            d = json.loads(line)  # more effecient way would be check the line validity before loading\n",
    "            if len(d) == 24 and len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "                \n",
    "                ## dow = day of week.  unpacking timestamp into individual vars \n",
    "                ##(or maybe I should do this in the next phase?  note this is still in unicode)\n",
    "                dow, month, day, time,_,year = d['created_at'].split(\" \") \n",
    "                ## Assign the extracted values into a new json to be stored.\n",
    "                             ## UserName --> from_user\n",
    "                processed = {\"from_user\":d['user']['screen_name'],\n",
    "                             ## Split hashtag, we only want the text in hashtag, discard indices.\n",
    "                             \"hashtag\":[hash_string['text'] for hash_string in d['entities']['hashtags']], \n",
    "                             ## Split terms in tweet text\n",
    "                             \"term\": d['text'].split(\" \"), \n",
    "                             ## append loc_ to each word in location\n",
    "                             \"location\":['loc_' + s for s in d['user']['location'].split(\" \")],\n",
    "                             ## mention ids\n",
    "                             \"mention\":d['entities']['user_mentions'],\n",
    "                             ## language\n",
    "                             \"lang\":d['lang'],\n",
    "                             ## day of week\n",
    "                             \"dow\":dow,\n",
    "                             ## month in 3 letters\n",
    "                             \"month\": month,\n",
    "                             ## numeric day 1-31\n",
    "                             \"day\":day,\n",
    "                             ## hr:min:sec\n",
    "                             \"time\":time,\n",
    "                             ## numeric year\n",
    "                             \"year\":year\n",
    "                            }\n",
    "                tweets.append(processed)\n",
    "                #tweets.append(d)\n",
    "            if counter % 50000 == 0:\n",
    "                print \"processed \", counter\n",
    "            counter+=1\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipyparallel\n",
      "  Downloading ipyparallel-6.0.2-py2.py3-none-any.whl (190kB)\n",
      "Requirement already satisfied: jupyter-client in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: decorator in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: ipython>=4 in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: ipython-genutils in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: ipykernel in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: pyzmq>=13 in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: tornado>=4 in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: futures; python_version == \"2.7\" in a:\\anaconda2\\lib\\site-packages (from ipyparallel)\n",
      "Requirement already satisfied: six>=1.5 in a:\\anaconda2\\lib\\site-packages (from python-dateutil>=2.1->ipyparallel)\n",
      "Installing collected packages: ipyparallel\n",
      "Successfully installed ipyparallel-6.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install ipyparallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseJson(jsonfile):\n",
    "    tweets = []\n",
    "    with gzip.open(jsonfile,'r') as fin:\n",
    "        counter = 0\n",
    "        for line in fin:\n",
    "            #if \"entities\" not in line:\n",
    "            #    continue\n",
    "            d = json.loads(line)  # more effecient way would be check the line validity before loading\n",
    "            if len(d) == 24 and len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "                \n",
    "                ## dow = day of week.  unpacking timestamp into individual vars \n",
    "                ##(or maybe I should do this in the next phase?  note this is still in unicode)\n",
    "                dow, month, day, time,_,year = d['created_at'].split(\" \") \n",
    "                ## Assign the extracted values into a new json to be stored.\n",
    "                             ## UserName --> from_user\n",
    "                processed = {\"from_user\":d['user']['screen_name'],\n",
    "                             ## Split hashtag, we only want the text in hashtag, discard indices.\n",
    "                             \"hashtag\":[hash_string['text'] for hash_string in d['entities']['hashtags']], \n",
    "                             ## Split terms in tweet text, remove \\n and \\r\n",
    "                             \"term\": [w.replace('\\n', '').replace('\\r','') for w in d['text'].split(\" \")], \n",
    "                             ## append loc_ to each word in location\n",
    "                             \"location\":['loc_' + s for s in d['user']['location'].split(\" \")],\n",
    "                             ## mention ids\n",
    "                             \"mention\":d['entities']['user_mentions'],\n",
    "                             ## language\n",
    "                             \"lang\":d['lang'],\n",
    "                             ## day of week\n",
    "                             \"dow\":dow,\n",
    "                             ## month in 3 letters\n",
    "                             \"month\": month,\n",
    "                             ## numeric day 1-31\n",
    "                             \"day\":day,\n",
    "                             ## hr:min:sec\n",
    "                             \"time\":time,\n",
    "                             ## numeric year\n",
    "                             \"year\":year\n",
    "                            }\n",
    "                tweets.append(processed)\n",
    "                #tweets.append(d)\n",
    "            if counter % 50000 == 0:\n",
    "                print \"processed \", counter\n",
    "            counter+=1\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "#import json\n",
    "import time\n",
    "import simplejson as json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Linux\n",
    "#jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "## Windows\n",
    "jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "path = os.path.join('..', 'data', 'batch1', *.gz')\n",
    "\n",
    "start_time = time.time()\n",
    "result = parseJson(jsonfilename)\n",
    "print('Pre-processing takes: {} seconds'.format(round(time.time() - start_time, 5)))\n",
    "\n",
    "#with open('data.txt', 'w') as outfile:\n",
    "#    json.dump(result, outfile)\n",
    "    \n",
    "with gzip.open('../out.json.gz', 'wb') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8413"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Linux\n",
    "#jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "\n",
    "## Windows\n",
    "jsonfilename = 'A:/statuses.log.2014-07-01-01.gz'\n",
    "\n",
    "start_time = time.time()\n",
    "result = parseJson(jsonfilename)\n",
    "print('Pre-processing takes: {} seconds'.format(round(time.time() - start_time, 5)))\n",
    "\n",
    "#with open('data.txt', 'w') as outfile:\n",
    "#    json.dump(result, outfile)\n",
    "    \n",
    "with gzip.open('../out.json.gz', 'wb') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ipcluster start -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipyparallel  import Client\n",
    "\n",
    "c = Client()\n",
    "v = c[:]\n",
    "#c.ids\n",
    "#c[:].apply_sync(lambda: \"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import time\n",
    "#import simplejson as json\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "def parseJson(jsonfile):\n",
    "    tweets = []\n",
    "    #counter = 0\n",
    "\n",
    "    with gzip.open(jsonfile,'r') as fin:\n",
    "        for line in fin:\n",
    "            #if \"entities\" not in line:\n",
    "            #    continue\n",
    "            #print(len(line))\n",
    "            if len(line) > 1000: # check if the line is proper tweet (by observation, all valid tweets have length at least 1000.)\n",
    "                d = json.loads(line.decode(\"UTF-8\"))  # more effecient way would be check the line validity before loading\n",
    "                #print(len(d))\n",
    "                ###### 21 in windows......this line is really ad hoc...but it works...\n",
    "                #if len(d) == 21 and len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "                #pprint(d)\n",
    "                if len(d['entities']['hashtags']): ## explicit booleaness to check is a list is empty\n",
    "        \n",
    "                    ## dow = day of week.  unpacking timestamp into individual vars \n",
    "                    ##(or maybe I should do this in the next phase?  note this is still in unicode)\n",
    "                    dow, month, day, time,_,year = d['created_at'].split(\" \") \n",
    "                    ## Assign the extracted values into a new json to be stored.\n",
    "                                 ## UserName --> from_user\n",
    "                    processed = {\"from_user\":d['user']['screen_name'],\n",
    "                                 ## Split hashtag, we only want the text in hashtag, discard indices.\n",
    "                                 \"hashtag\":[hash_string['text'] for hash_string in d['entities']['hashtags']], \n",
    "                                 ## Split terms in tweet text, remove \\n and \\r\n",
    "                                 \"term\": [w.replace('\\n', '').replace('\\r','') for w in d['text'].split(\" \")], \n",
    "                                 ## append loc_ to each word in location\n",
    "                                 \"location\":['loc_' + s for s in d['user']['location'].split(\" \")],\n",
    "                                 ## mention ids\n",
    "                                 \"mention\":d['entities']['user_mentions'],\n",
    "                                 ## language\n",
    "                                 \"lang\":d['user']['lang'],\n",
    "                                 ## day of week\n",
    "                                 \"dow\":dow,\n",
    "                                 ## month in 3 letters\n",
    "                                 \"month\": month,\n",
    "                                 ## numeric day 1-31\n",
    "                                 \"day\":day,\n",
    "                                 ## hr:min:sec\n",
    "                                 \"time\":time,\n",
    "                                 ## numeric year\n",
    "                                 \"year\":year\n",
    "                                }\n",
    "                    \n",
    "                    tweets.append(processed)\n",
    "                    #tweets.append(d)\n",
    "                #if counter % 100000 == 0:\n",
    "                #    print \"processed \", counter\n",
    "                #counter+=1\n",
    "    return tweets\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('running folder ', '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_2_8/2013-02/2013-02-01', '...')\n",
      "Pre-processing /mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_2_8/2013-02/2013-02-01 takes - 0:00:05 which is 5 seconds in total\n",
      "6586\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print(sys.argv)\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## Linux\n",
    "    #jsonfilename = '/home/danielshi/tw_pyspark/Sample/statuses.log.2014-07-01-00.gz'\n",
    "    folder = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_2_8/2013-02/2013-02-01'    #'/mnt/f/Demoonism/Data Scientist/Spark/statuses.log.2014-07-01-00.gz'\n",
    "    # /mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_2_8/2013-02/2013-02-01\n",
    "    print(\"running folder \",folder, \"...\" )\n",
    "    ## Windows\n",
    "    #jsonfilename = '../statuses.log.2014-07-01-00.gz'\n",
    "    path = os.path.join(folder,'*.gz')\n",
    "    output = []\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    counter = 0\n",
    "    for jsonfilename in files[:1]:\n",
    "        #print \"processing \", counter\n",
    "        result = parseJson(jsonfilename)\n",
    "        #print result[:1]\n",
    "        #output.append(result)\n",
    "        output = output + result ## simple concatenation\n",
    "        counter += 1\n",
    "        if counter % 2 == 0:\n",
    "            print \"processed \", counter\n",
    "        out_filename = '../output/out'\n",
    "        #out_filename = 'out'\n",
    "        sec = time.time() - start_time\n",
    "        m, s = divmod(sec, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print('Pre-processing '+ folder +' takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))\n",
    "        print(len(output[0]))\n",
    "        with gzip.open(out_filename+'.json.gz', 'wb') as f:\n",
    "            json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
